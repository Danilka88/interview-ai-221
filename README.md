# AI Interviewer: Голосовой ассистент и система анализа резюме

Это комплексное веб-приложение, предназначенное для проведения и симуляции собеседований, а также для автоматического скоринга и ранжирования резюме. Система может работать как через веб-интерфейс, так и в качестве API-сервиса для интеграции с внешними системами (например, HR-платформами).

## Ключевые возможности

- **Голосовое собеседование:** Полноценный диалог с AI-рекрутером в режиме реального времени. Поддерживает несколько режимов: с использованием локального Vosk, с настраиваемым STT-провайдером (Google Cloud, Yandex SpeechKit и др.) и с предварительной обработкой аудио (шумоподавление).
- **Текстовая симуляция:** Наблюдение за диалогом между двумя ИИ (рекрутером и кандидатом).
- **Скоринг и ранжирование резюме:** Автоматическая оценка и сортировка кандидатов на основе текста вакансии и их резюме, **с учетом настраиваемых весов критериев**.
- **Конструктор вакансий:** Инструмент для генерации улучшенных описаний вакансий с помощью LLM, учитывающий заданные критерии и их веса.
- **Динамический контекст:** Возможность загружать файлы с описанием вакансии и резюме для персонализации анализа и генерации вопросов/описаний.
- **Дашборд и управление кандидатами:** Просмотр всех вакансий, привязанных кандидатов и их интервью. Возможность обновления статуса кандидата в воронке найма.
- **Webhook API:** Предоставление всего функционала (ранжирование резюме, симуляция интервью, генерация описаний вакансий, добавление вакансий и кандидатов в БД, генерация тегов) через защищенное API для интеграции со сторонними сайтами (например, Laravel). Задачи выполняются асинхронно с обратным вызовом (вебхуком).
- **Гибкая интеграция STT-сервисов:** Возможность выбора и настройки различных провайдеров распознавания речи (Speech-to-Text), включая локальный Vosk и облачные сервисы, такие как Google Cloud Speech-to-Text и Yandex SpeechKit.
- **Предварительная обработка аудио:** Новый модуль для шумоподавления аудио перед передачей в STT. Настраивается через отдельную страницу (`/audio-processing/settings`) и может быть интегрирован в голосовое собеседование.
- **LLM Провайдеры:** Новый модуль для подключения сторонних LLM API (OpenAI, YandexGPT, Sber GigaChat) через LangChain. Позволяет использовать различные LLM для генерации текста. Функционал доступен через отдельную страницу настроек (`/llm-providers/settings`), а также через специальный API (`/api/v1/llm-providers/generate`) и Webhook (`/api/v1/llm-providers/webhook/generate`), не затрагивая основной LLM приложения.
- **Интеграция речи:** Использует офлайн-модель Vosk для распознавания и Silero для синтеза речи.

## Технологический стек

- **Бэкенд:** Python, FastAPI, LangChain, Uvicorn, websockets, aiofiles, Pydantic-settings, Pydantic, httpx, **SQLAlchemy (для ORM), SQLite (база данных)**
- **Фронтенд:** HTML5, Pico.css, ванильный JavaScript
- **LLM:** Ollama (гибкая настройка моделей через `.env`)
- **Распознавание речи (STT):** Гибкая архитектура с поддержкой Vosk (локально) и возможностью интеграции облачных сервисов (Google Cloud Speech-to-Text, Yandex SpeechKit).
- **Синтез речи (TTS):** Silero Models

## Установка и запуск

### 1. Предварительные требования

- **Python:** Убедитесь, что у вас установлен Python версии 3.10 или выше.
- **Ollama:** У вас должна быть установлена и запущена платформа [Ollama](https://ollama.com/).

### 2. Установка проекта

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL вашего репозитория>
    cd <название папки проекта>
    ```

2.  **Создайте и активируйте виртуальное окружение:**
    ```bash
    # Для macOS/Linux
    python3 -m venv venv
    source venv/bin/activate

    # Для Windows
    python -m venv venv
    venv\Scripts\activate
    ```

3.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```

### 3. Инициализация базы данных

При первом запуске или при изменении моделей данных необходимо создать таблицы в базе данных.
Выполните следующую команду:

```bash
python main.py init_db
```
Эта команда создаст файл `interview_ai.db` в корне проекта и все необходимые таблицы.

### 4. Настройка окружения

1.  **Создайте файл `.env`** в корне проекта, скопировав в него содержимое шаблона ниже.
2.  **Настройте переменные:** Замените значения на свои, если это необходимо. Особенно важно установить надежный `WEBHOOK_SECRET_TOKEN`.

    **Шаблон `.env.example`:**
    ```env
    # Переменные окружения для AI Interviewer

    # Настройки моделей LLM для Ollama
    LLM_INTERVIEWER_MODEL="gemma3:4b"
    LLM_CANDIDATE_MODEL="qwen2.5-coder:3b"
    LLM_QUESTION_GEN_MODEL="gemma3:4b"
    LLM_ANALYST_MODEL="gemma3:4b" # Используется для анализа, скоринга и генерации описаний вакансий
    LLM_TEXT_MODEL="gemma3:4b" # Используется для генерации текста без JSON формата (например, в конструкторе вакансий)

    # Настройки моделей обработки голоса
    VOSK_MODEL_PATH="vosk-model-ru"
    SILERO_MODEL_PATH="v3_1_ru.pt"

    # Секретный токен для Webhook API. ВАЖНО: Замените на свое уникальное, сложное значение!
    WEBHOOK_SECRET_TOKEN="your-super-secret-and-long-token-here"
    ```

3.  **Скачайте AI-модели:** Убедитесь, что все модели, указанные в `.env`, скачаны в Ollama. Пример:
    ```bash
    ollama pull gemma3:4b
    ollama pull qwen2.5-coder:3b
    ```

4.  **Скачайте модель распознавания речи (для Vosk):**
    - Перейдите на [страницу моделей Vosk](https://alphacephei.com/vosk/models) и скачайте модель для русского языка (например, `vosk-model-ru-0.22`).
    - Распакуйте архив и убедитесь, что его содержимое находится в папке `vosk-model-ru` в корне вашего проекта.

*Примечание: Модель для синтеза речи (Silero) будет скачана автоматически при первом запуске сервера.*

### 5. Настройка облачных STT-сервисов (опционально)

Если вы планируете использовать облачные провайдеры STT (Google Cloud Speech-to-Text, Yandex SpeechKit и т.д.), вам необходимо получить соответствующие API ключи и настроить их в файле `.env`.

*   **Google Cloud Speech-to-Text:**
    1.  Создайте проект в [Google Cloud Console](https://console.cloud.google.com/).
    2.  Включите API "Cloud Speech-to-Text API".
    3.  Создайте учетные данные (API ключ) и скопируйте его.
    4.  Вставьте ключ в переменную `GOOGLE_CLOUD_SPEECH_API_KEY` в вашем файле `.env`.
*   **Yandex SpeechKit:**
    1.  Создайте аккаунт в [Яндекс.Облаке](https://cloud.yandex.ru/).
    2.  Создайте платежный аккаунт и привяжите карту (для использования SpeechKit).
    3.  Создайте сервисный аккаунт и выдайте ему роль `ai.speechkit.user`.
    4.  Сгенерируйте API-ключ для сервисного аккаунта.
    5.  Вставьте ключ в переменную `YANDEX_SPEECHKIT_API_KEY` в вашем файле `.env`.

*Важно: Никогда не делитесь вашими API ключами и не коммитьте их в публичные репозитории.*

### 5. Запуск приложения

Убедитесь, что Ollama запущена. Затем выполните команду:
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```
Приложение будет доступно в вашем браузере по адресу `http://localhost:8000`.

## Использование

### Веб-интерфейс

1.  Откройте `http://localhost:8000` в браузере. Это приведет вас на страницу **"Дашборд"**.
2.  Используйте навигационное меню для выбора режима. Основные разделы:
    *   **Дашборд (`/` или `/dashboard`):** Главная страница приложения. Позволяет просматривать все загруженные вакансии, привязанных к ним кандидатов и результаты их интервью. Вы можете обновлять статус кандидата (например, "new", "interview_completed", "hired", "rejected") и запускать новые собеседования для кандидатов, которые еще не прошли интервью.
    *   **Отбор резюме (`/rank`):** Инструмент для автоматического скоринга и ранжирования резюме.
    *   **Конструктор вакансий (`/vacancy_builder`):** Инструмент для генерации улучшенных описаний вакансий.
    *   **Текстовая симуляция (`/test`):** Наблюдение за диалогом между двумя ИИ (рекрутером и кандидатом).
    *   **Последний отчет (`/result`):** Просмотр последнего сгенерированного отчета.
3.  **Настройка STT-провайдера:**
    *   Перейдите на страницу **"Настройки STT-провайдера"** (`/stt-settings`) через меню.
    *   Здесь вы можете выбрать провайдера распознавания речи (Vosk, Google Cloud, Yandex SpeechKit) и ввести необходимые API ключи.
    *   После сохранения настроек вы сможете начать голосовое собеседование, используя выбранный STT-провайдер.
*   **Настройка обработки аудио:**
    *   Перейдите на страницу **"Настройки обработки аудио"** (`/audio-processing/settings`) через меню.
    *   Здесь вы можете включить/выключить шумоподавление и настроить его интенсивность.
    *   Эти настройки будут применяться к аудиопотоку при использовании нового WebSocket эндпоинта `/ws/live_processed`.
*   **Настройка LLM-провайдеров:**
    *   Перейдите на страницу **"Настройки LLM-провайдеров"** (`/llm-providers/settings`) через меню.
    *   Здесь вы можете выбрать сторонний LLM-провайдер (OpenAI, YandexGPT, Sber GigaChat) и ввести необходимые API ключи.
    *   Этот функционал доступен только через отдельный API и Webhook, не затрагивая основной LLM приложения.
4.  **Голосовое собеседование:** AI-рекрутер теперь получает только краткое саммари технических требований вакансии, что помогает ему фокусироваться на ключевых аспектах без "лишнего мусора".
5.  **Сохранение контекста:** Сгенерированные вопросы и настроенные веса теперь сохраняются в базе данных вместе с вакансией и доступны для использования при повторном запуске интервью с дашборда.

### Webhook API

Проект предоставляет асинхронное API для интеграции со сторонними системами. Все эндпоинты защищены токеном.

**Доступные Webhook-эндпоинты:**
- `POST /api/v1/webhook/rank-resumes`: Запускает асинхронное ранжирование резюме относительно вакансии с учетом весов.
- `POST /api/v1/webhook/start-interview`: Инициирует полную симуляцию текстового интервью между AI-рекрутером и AI-кандидатом.
- `POST /api/v1/webhook/build-vacancy`: Генерирует улучшенное описание вакансии на основе черновика и заданных весов.
- `POST /api/v1/webhook/add-vacancy-with-candidates`: Сохраняет новую вакансию и привязанных к ней кандидатов в базу данных.
- `POST /api/v1/webhook/generate-tags`: Извлекает ключевые навыки и характеристики из текста вакансии в виде облака тегов.

Все запросы к этим эндпоинтам должны содержать заголовок `X-Webhook-Token` с секретным токеном, указанным в вашем `.env`. После получения запроса API немедленно отвечает `202 Accepted`, а результат выполнения задачи отправляется на `webhook_url`, указанный в теле запроса.

Подробное описание методов, моделей данных и примеры запросов находятся в файле **[WEBHOOK_API.md](WEBHOOK_API.md)**.
