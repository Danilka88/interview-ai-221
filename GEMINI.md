## 1. Обзор проекта

**AI Interviewer** — это комплексное веб-приложение на Python и FastAPI для проведения и анализа собеседований. Система может работать как через веб-интерфейс, так и в качестве **API-сервиса** для интеграции с внешними системами (например, HR-платформами).

Ключевые возможности:
- Проведение собеседований в нескольких режимах (голосовой чат, текстовая симуляция, стресс-тест симуляция).
- Визуализация результатов: Интерактивные графики (круговые, радарные, столбчатые) с использованием Chart.js для наглядного представления результатов анализа собеседований и резюме.
- Расширенная поддержка форматов файлов: Возможность загрузки и обработки резюме и вакансий в форматах PDF, DOCX и RTF, помимо TXT и MD.
- Автоматический скоринг и ранжирование резюме относительно вакансии, **с учетом настраиваемых весов критериев**.
- **Конструктор вакансий:** Инструмент для генерации улучшенных описаний вакансий с помощью LLM, учитывающий заданные критерии и их веса.
- Динамическая адаптация к контексту (загруженные вакансии, резюме).
- **Дашборд:** Инструмент для просмотра всех вакансий, привязанных кандидатов и их интервью, а также для управления статусами кандидатов. **Является главной страницей приложения.**
- **Асинхронный Webhook API** для интеграции со сторонними сервисами, предоставляющий функционал ранжирования резюме, симуляции интервью, генерации описаний вакансий, добавления вакансий и кандидатов в БД, генерации тегов.
- **Гибкая интеграция STT-сервисов:** Возможность выбора и настройки различных провайдеров распознавания речи (Speech-to-Text), включая локальный Vosk и облачные сервисы, такие как Google Cloud Speech-to-Text и Yandex SpeechKit.
- **Улучшенное голосовое собеседование:** AI-reкрутер теперь получает только краткое саммари технических требований вакансии, что помогает ему фокусироваться на ключевых аспектах без "лишнего мусора", используя настраиваемый STT-провайдер.
- **Сохранение контекста:** Сгенерированные вопросы и настроенные веса теперь сохраняются в базе данных вместе с вакансией и доступны для использования при повторном запуске интервью с дашборда.

## 2. Технологический стек

- **Бэкенд:** Python, FastAPI, Uvicorn, Pydantic, **SQLAlchemy, SQLite**, pypdf, python-docx, striprtf
- **Frontend:** Ванильный HTML/CSS/JavaScript с использованием CSS-фреймворка Pico.css, Chart.js (для визуализации данных).
- **Взаимодействие с ИИ:** LangChain для управления цепочками вызовов и промптами.
- **Языковые модели (LLM):** Ollama для локального запуска моделей или OpenRouter (через коннектор) для облачных моделей.
- **Речь:** Гибкая архитектура с поддержкой Vosk (локально) и возможностью интеграции облачных сервисов (Google Cloud Speech-to-Text, Yandex SpeechKit) для распознавания речи (STT), а также Silero Models для синтеза речи (TTS).
- **Управление конфигурацией:** Pydantic-settings для чтения настроек из `.env` файла.
- **Асинхронный ввод-вывод:** `aiofiles` для неблокирующей работы с файлами.
- **HTTP-клиент:** `httpx` для отправки исходящих вебхуков.

## 3. Принципы и лучшие практики

Этот проект следует строгим принципам для обеспечения качества и поддерживаемости. При любой доработке необходимо соблюдать эти правила.

1.  **Конфигурация через `.env`**: Вся конфигурация (имена моделей, пути, токены) находится **исключительно** в файле `.env`. Доступ к настройкам осуществляется через объект `settings` из `core.config`.
2.  **Использование базы данных и ORM**: Для хранения данных используется SQLite с SQLAlchemy в качестве ORM. Все взаимодействия с БД должны быть асинхронными.

3.  **Полная асинхронность**: Все операции ввода-вывода (работа с файлами, сетевые запросы) **обязаны** быть асинхронными (`async/await`, `aiofiles`, `httpx`).
4.  **Строгое разделение логики**: Архитектура "тонкие контроллеры, толстые сервисы".
    - **`api/`**: Содержит только логику обработки HTTP-запросов и валидации. Не содержит бизнес-логики.
    - **`services/`**: Содержит всю бизнес-логику (вызовы к ИИ, обработка данных). Не зависит от FastAPI.
5.  **Централизация промптов**: Все системные промпты для LLM хранятся в виде констант в директории `prompts/`.
6.  **Безопасность API**: Внешний API должен быть защищен. В данном проекте используется проверка по секретному токену в заголовке `X-Webhook-Token`.
7.  **Настраиваемые веса критериев оценки**: Система позволяет гибко настраивать веса для различных критериев оценки (например, технические навыки, коммуникация, кейсы). Эти веса учитываются LLM при формировании оценки или генерации описаний **для всех сценариев: конструктор вакансий, скоринг резюме и анализ интервью**.
8.  **Полноценная динамическая адаптация интервью**: LLM используется для динамической генерации вопросов на лету, учитывая контекст диалога, цели интервью, информацию о кандидате и вакансии. Это обеспечивает более глубокую и гибкую логику ветвления и генерации вопросов.
9.  **Модульность и расширяемость внешних сервисов**: Интеграция со сторонними сервисами (например, STT-провайдерами) должна быть реализована через абстрактные интерфейсы, позволяющие легко добавлять или заменять реализации без изменения основной бизнес-логики.

## 4. Ключевые сценарии использования

### Сценарий 1: Ранжирование резюме (через веб-интерфейс)

1.  **Пользователь** открывает `/rank`, загружает вакансию и резюме (поддерживаются форматы TXT, MD, PDF, DOCX, RTF).
2.  **Frontend** отправляет файлы на `POST /rank-resumes`.
3.  **Бэкенд (`api/ranking.py`)** вызывает сервис `score_and_sort_resumes`, который обрабатывает каждый файл и возвращает отсортированный список с результатами и ошибками.
4.  **Frontend** получает JSON, сохраняет в `localStorage` и перенаправляет на `/rank/result` для отрисовки и визуализации результатов с Chart.js.

### Сценарий 2: Проведение собеседования (через веб-интерфейс)

1.  **Пользователь** открывает `/voice-interview` (голосовое) или `/test` (текстовое), опционально загружает файлы (вакансия, резюме, поддерживаются форматы TXT, MD, PDF, DOCX, RTF).
2.  **Frontend** сохраняет контекст в `interviewContext`.
3.  При старте устанавливается WebSocket-соединение (`/ws/live` или `/ws/test`), и на сервер отправляется `interviewContext`.
4.  **Бэкенд (`api/interview.py`)** начинает диалог. **Первый вопрос AI-рекрутера включает приветствие, название вакансии и краткий обзор тем из резюме/вакансии, которые будут обсуждаться.**
5.  По завершении **фронтенд** сохраняет результат в `localStorage` и переходит на `/result` для финального анализа и визуализации с Chart.js.

### Сценарий 3: Ранжирование резюме (через Webhook API)

1.  **Внешний сервис** (например, Laravel) отправляет запрос на `POST /api/v1/webhook/rank-resumes`, передавая в теле JSON `webhook_url`, `vacancy_text`, список резюме и в заголовке `X-Webhook-Token`.
2.  **API (`api/webhook.py`)** проверяет токен и, если он валиден, немедленно отвечает `202 Accepted` и запускает фоновую задачу `process_ranking_request`.
3.  **Фоновая задача (`services/webhook_service.py`)** вызывает сервис `score_and_sort_resumes` для обработки данных.
4.  По завершении **сервис** формирует итоговый JSON и с помощью `httpx` отправляет его на `webhook_url`, подписав запрос HMAC-подписью для проверки подлинности.

### Сценарий 4: Конструктор вакансий (через веб-интерфейс)

1.  **Пользователь** открывает `/vacancy_builder`, загружает базовый текст вакансии и настраивает веса критериев (например, технические навыки, коммуникация).
2.  **Frontend** отправляет текст вакансии и настроенные веса на `POST /api/v1/build-vacancy-description`.
3.  **Бэкенд (`api/general.py`)** вызывает сервис `build_vacancy_description`, который использует LLM для генерации улучшенного описания вакансии, учитывая заданные веса.
4.  **Frontend** получает сгенерированное описание (в формате Markdown), отображает его для предварительного просмотра и предоставляет опции для скачивания в форматах `.md` и `.json`.

### Сценарий 5: Управление кандидатами и дашборд

1.  **Пользователь** открывает `/dashboard`.
2.  **Frontend** отправляет запрос на `GET /dashboard/data` для получения всех вакансий с привязанными кандидатами и их интервью.
3.  **Бэкенд (`api/dashboard.py`)** возвращает структурированные данные.
4.  **Frontend** отображает данные в виде таблицы или списка.
5.  **Пользователь** может изменить статус кандидата (например, "new", "interview_completed", "hired", "rejected") через интерфейс.
6.  **Frontend** отправляет запрос на `PUT /candidate/{candidate_id}/status`.
7.  **Бэкенд (`api/dashboard.py`)** обновляет статус кандидата в базе данных.

### Сценарий 6: Симуляция интервью (через Webhook API)

1.  **Внешний сервис** отправляет запрос на `POST /api/v1/webhook/start-interview`, передавая в теле JSON `webhook_url`, `vacancy_text`, `resume_text` и в заголовке `X-Webhook-Token`.
2.  **API (`api/api_v1.py`)** проверяет токен и, если он валиден, немедленно отвечает `202 Accepted` и запускает фоновую задачу `process_interview_simulation_request`.
3.  **Фоновая задача (`services/api_webhook_service.py`)** вызывает сервис `run_interview_simulation` для проведения симуляции.
4.  По завершении **сервис** формирует итоговый JSON с историей чата и анализом, и с помощью `httpx` отправляет его на `webhook_url`.

### Сценарий 7: Конструктор вакансий (через Webhook API)

1.  **Внешний сервис** отправляет запрос на `POST /api/v1/webhook/build-vacancy`, передавая в теле JSON `webhook_url`, `base_text`, `weights` и в заголовке `X-Webhook-Token`.
2.  **API (`api/api_v1.py`)** проверяет токен и, если он валиден, немедленно отвечает `202 Accepted` и запускает фоновую задачу `process_vacancy_build_request`.
3.  **Фоновая задача (`services/api_webhook_service.py`)** вызывает сервис `build_vacancy_description` для генерации описания.
4.  По завершении **сервис** формирует итоговый JSON с описанием вакансии и с помощью `httpx` отправляет его на `webhook_url`.

### Сценарий 8: Добавление вакансии и кандидатов (через Webhook API)

1.  **Внешний сервис** отправляет запрос на `POST /api/v1/webhook/add-vacancy-with-candidates`, передавая в теле JSON `webhook_url`, `vacancy_filename`, `vacancy_content`, `resumes` и в заголовке `X-Webhook-Token`.
2.  **API (`api/api_v1.py`)** проверяет токен и, если он валиден, немедленно отвечает `202 Accepted` и запускает фоновую задачу `process_add_vacancy_request`.
3.  **Фоновая задача (`services/api_webhook_service.py`)** вызывает сервисы для сохранения вакансии и кандидатов в БД.
4.  По завершении **сервис** формирует итоговый JSON с сообщением об успехе и `vacancy_id`, и с помощью `httpx` отправляет его на `webhook_url`.

### Сценарий 9: Генерация тегов для вакансии (через Webhook API)

1.  **Внешний сервис** отправляет запрос на `POST /api/v1/webhook/generate-tags`, передавая в теле JSON `webhook_url`, `vacancy_text` и в заголовке `X-Webhook-Token`.
2.  **API (`api/api_v1.py`)** проверяет токен и, если он валиден, немедленно отвечает `202 Accepted` и запускает фоновую задачу `process_tag_generation_request`.
3.  **Фоновая задача (`services/api_webhook_service.py`)** вызывает сервис `generate_tags_for_vacancy` для извлечения тегов.
4.  По завершении **сервис** формирует итоговый JSON с тегами и с помощью `httpx` отправляет его на `webhook_url`.

### Сценарий 10: Настройка и тестирование STT-провайдера

1.  **Пользователь** открывает `/stt-settings` через веб-интерфейс.
2.  **Frontend** отправляет `GET /api/v1/stt-config` для получения текущих настроек STT.
3.  **Пользователь** выбирает провайдера (например, Google Cloud) и вводит API ключ.
4.  **Frontend** отправляет `POST /api/v1/stt-config` с новыми настройками.
5.  **Бэкенд (`api/stt_settings.py`)** обновляет настройки STT в `SettingsManager` (в памяти).
6.  **Пользователь** нажимает "Проверить настройки STT".
7.  **Frontend** отправляет `POST /api/v1/stt-test` с текущими настройками.
8.  **Бэкенд (`api/stt_settings.py`)** пытается инициализировать распознаватель выбранного провайдера, чтобы проверить корректность настроек (например, наличие API ключа).
9.  **Бэкенд** возвращает результат теста.
10. **Пользователь** может нажать "Начать собеседование с выбранным STT", чтобы перейти на `/stt-voice-interview` и использовать выбранный провайдер.

### Сценарий 11: Стресс-тест симуляция (через веб-интерфейс)

1.  **Пользователь** открывает `/stress-simulation` через веб-интерфейс.
2.  **Frontend** отправляет запрос на `POST /api/v1/stress-simulation` с контекстом (вакансия, резюме).
3.  **Бэкенд (`api/interview.py`)** запускает симуляцию интервью, где AI-рекрутер взаимодействует со "сложным" кандидатом.
4.  По завершении **фронтенд** сохраняет результат в `localStorage` и переходит на `/result` для финального анализа и визуализации с Chart.js.

## 5. Запуск и отладка

Для корректной работы и отладки приложения следуйте этим шагам.

**1. Установка зависимостей:**

```bash
pip install -r requirements.txt
```

**2. Инициализация базы данных:**

При первом запуске или при изменении моделей данных необходимо создать таблицы в базе данных.
Выполните следующую команду:

```bash
python main.py init_db
```
Эта команда создаст файл `interview_ai.db` в корне проекта и все необходимые таблицы.

**3. Настройка окружения:**

**2. Настройка окружения:**

- Создайте файл `.env` в корне проекта (можно скопировать из `README.md`).
- Убедитесь, что у вас **запущен локальный сервер Ollama** ИЛИ **коннектор OpenRouter** (если вы используете облачные модели).
- Убедитесь, что модели, указанные в `.env`, скачаны в Ollama (`ollama pull ...`).

**4. Запуск сервера FastAPI:**

Для разработки и отладки сервер следует запускать в **интерактивном режиме**.

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```
